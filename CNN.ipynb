{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "BATCH_SIZE=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 数据提取和划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 156057, 156058, 156059])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv(\"train.tsv\",sep='\\t')\n",
    "idx=np.arange(train_data.shape[0])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv(\"test.tsv\",sep='\\t')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集、开发集和测试集的大小\n",
    "np.random.shuffle(idx)\n",
    "train_size=int(len(idx)*0.6)\n",
    "test_size=int(len(idx)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将原数据集划分，生成各自的文件\n",
    "train_data.iloc[idx[:train_size], :].to_csv('data/cnn_train.csv',index=False)\n",
    "train_data.iloc[idx[train_size:test_size], :].to_csv(\"data/cnn_test.csv\", index=False)\n",
    "train_data.iloc[idx[test_size:], :].to_csv(\"data/cnn_dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"data/cnn_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext加载数据\n",
    "from torchtext import data\n",
    "TEXT = data.Field(sequential=True,batch_first=True,lower=True)\n",
    "LABEL =data.Field(sequential=False,batch_first=True,unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "datafields = [# 不需要的filed设置为None\n",
    "    (\"PhraseId\", None), \n",
    "    (\"SentenceId\", None),\n",
    "    ('Phrase', TEXT),\n",
    "    ('Sentiment', LABEL)\n",
    "]\n",
    "datafields2 = [# 不需要的filed设置为None\n",
    "    (\"PhraseId\", None), \n",
    "    (\"SentenceId\", None),\n",
    "    ('Phrase', TEXT),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TabularDataset(path='data/cnn_train.csv',format='csv',fields=datafields,skip_header=True)\n",
    "dev_data=data.TabularDataset(path='data/cnn_dev.csv',format='csv',fields=datafields,skip_header=True)\n",
    "test_data=data.TabularDataset(path='data/cnn_test.csv',format='csv',fields=datafields,skip_header=True)\n",
    "pred=data.TabularDataset(path='data/cnn_pred.csv',format='csv',fields=datafields2,skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,vectors='glove.6B.50d',unk_init= lambda x:torch.nn.init.uniform_(x, a=-0.25, b=0.25))\n",
    "LABEL.build_vocab(train_data)\n",
    "PAD_INDEX = TEXT.vocab.stoi['<pad>']\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代器\n",
    "train_iterator = data.BucketIterator(train_data,batch_size=BATCH_SIZE,train=True,shuffle=True)\n",
    "dev_iterator = data.BucketIterator(dev_data,batch_size=len(dev_data),train=False,sort=False)\n",
    "test_iterator = data.BucketIterator(test_data,batch_size=len(test_data),train=False,sort=False)\n",
    "pred_iterator = data.BucketIterator(pred,batch_size=len(pred),train=False,sort=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16456, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置参数\n",
    "embedding_choice='glove'\n",
    "num_embeddings=len(TEXT.vocab)\n",
    "embedding_dim=50\n",
    "dropoutp=0.5\n",
    "num_filter=100 #卷积核个数\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "vocab_size,label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN实现\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, \n",
    "                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "        \n",
    "        # 三个不同大小的2维卷积核\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=num_filter,kernel_size=(3,embedding_dim),padding=(2,0))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1,out_channels=num_filter,kernel_size=(4,embedding_dim),padding=(3,0))\n",
    "        self.conv3 = nn.Conv2d(in_channels=1,out_channels=num_filter,kernel_size=(5,embedding_dim),padding=(4,0))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropoutp)\n",
    "        self.fc = nn.Linear(num_filter*3,label_num)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x)).squeeze(3)\n",
    "        x1 = F.max_pool1d(x1,x1.size(2)).squeeze(2)\n",
    "        x2 = F.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = F.max_pool1d(x2,x2.size(2)).squeeze(2)\n",
    "        x3 = F.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = F.max_pool1d(x3,x3.size(2)).squeeze(2)\n",
    "        \n",
    "        x = torch.cat((x1,x2,x3),dim=1)\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_1.068%:  Training average Loss: 1.342183\n",
      "Epoch 0_2.136%:  Training average Loss: 1.311262\n",
      "Epoch 0_3.204%:  Training average Loss: 1.280759\n",
      "Epoch 0_4.272%:  Training average Loss: 1.247747\n",
      "Epoch 0_5.340%:  Training average Loss: 1.224701\n",
      "Epoch 0_6.408%:  Training average Loss: 1.201019\n",
      "Epoch 0_7.476%:  Training average Loss: 1.192756\n",
      "Epoch 0_8.544%:  Training average Loss: 1.180743\n",
      "Epoch 0_9.612%:  Training average Loss: 1.175871\n",
      "Epoch 0_10.680%:  Training average Loss: 1.169905\n",
      "Epoch 0_11.748%:  Training average Loss: 1.159898\n",
      "Epoch 0_12.816%:  Training average Loss: 1.153282\n",
      "Epoch 0_13.884%:  Training average Loss: 1.147058\n",
      "Epoch 0_14.952%:  Training average Loss: 1.145108\n",
      "Epoch 0_16.019%:  Training average Loss: 1.136890\n",
      "Epoch 0_17.087%:  Training average Loss: 1.131737\n",
      "Epoch 0_18.155%:  Training average Loss: 1.125916\n",
      "Epoch 0_19.223%:  Training average Loss: 1.120772\n",
      "Epoch 0_20.291%:  Training average Loss: 1.117637\n",
      "Epoch 0_21.359%:  Training average Loss: 1.114275\n",
      "Epoch 0_22.427%:  Training average Loss: 1.112206\n",
      "Epoch 0_23.495%:  Training average Loss: 1.111028\n",
      "Epoch 0_24.563%:  Training average Loss: 1.106800\n",
      "Epoch 0_25.631%:  Training average Loss: 1.102843\n",
      "Epoch 0_26.699%:  Training average Loss: 1.100690\n",
      "Epoch 0_27.767%:  Training average Loss: 1.099114\n",
      "Epoch 0_28.835%:  Training average Loss: 1.098155\n",
      "Epoch 0_29.903%:  Training average Loss: 1.096934\n",
      "Epoch 0_30.971%:  Training average Loss: 1.095080\n",
      "Epoch 0_32.039%:  Training average Loss: 1.094085\n",
      "Epoch 0_33.107%:  Training average Loss: 1.092500\n",
      "Epoch 0_34.175%:  Training average Loss: 1.091018\n",
      "Epoch 0_35.243%:  Training average Loss: 1.090063\n",
      "Epoch 0_36.311%:  Training average Loss: 1.090249\n",
      "Epoch 0_37.379%:  Training average Loss: 1.090031\n",
      "Epoch 0_38.447%:  Training average Loss: 1.089026\n",
      "Epoch 0_39.515%:  Training average Loss: 1.089096\n",
      "Epoch 0_40.583%:  Training average Loss: 1.087224\n",
      "Epoch 0_41.651%:  Training average Loss: 1.086709\n",
      "Epoch 0_42.719%:  Training average Loss: 1.085244\n",
      "Epoch 0_43.787%:  Training average Loss: 1.084756\n",
      "Epoch 0_44.855%:  Training average Loss: 1.084123\n",
      "Epoch 0_45.923%:  Training average Loss: 1.083988\n",
      "Epoch 0_46.990%:  Training average Loss: 1.083817\n",
      "Epoch 0_48.058%:  Training average Loss: 1.081762\n",
      "Epoch 0_49.126%:  Training average Loss: 1.080998\n",
      "Epoch 0_50.194%:  Training average Loss: 1.081089\n",
      "Epoch 0_51.262%:  Training average Loss: 1.080744\n",
      "Epoch 0_52.330%:  Training average Loss: 1.081018\n",
      "Epoch 0_53.398%:  Training average Loss: 1.079930\n",
      "Epoch 0_54.466%:  Training average Loss: 1.080283\n",
      "Epoch 0_55.534%:  Training average Loss: 1.079267\n",
      "Epoch 0_56.602%:  Training average Loss: 1.078831\n",
      "Epoch 0_57.670%:  Training average Loss: 1.077441\n",
      "Epoch 0_58.738%:  Training average Loss: 1.076818\n",
      "Epoch 0_59.806%:  Training average Loss: 1.075791\n",
      "Epoch 0_60.874%:  Training average Loss: 1.075100\n",
      "Epoch 0_61.942%:  Training average Loss: 1.074862\n",
      "Epoch 0_63.010%:  Training average Loss: 1.074093\n",
      "Epoch 0_64.078%:  Training average Loss: 1.072944\n",
      "Epoch 0_65.146%:  Training average Loss: 1.072526\n",
      "Epoch 0_66.214%:  Training average Loss: 1.071606\n",
      "Epoch 0_67.282%:  Training average Loss: 1.070602\n",
      "Epoch 0_68.350%:  Training average Loss: 1.070404\n",
      "Epoch 0_69.418%:  Training average Loss: 1.070227\n",
      "Epoch 0_70.486%:  Training average Loss: 1.069839\n",
      "Epoch 0_71.554%:  Training average Loss: 1.069435\n",
      "Epoch 0_72.622%:  Training average Loss: 1.069475\n",
      "Epoch 0_73.690%:  Training average Loss: 1.069173\n",
      "Epoch 0_74.758%:  Training average Loss: 1.068750\n",
      "Epoch 0_75.826%:  Training average Loss: 1.068148\n",
      "Epoch 0_76.894%:  Training average Loss: 1.067796\n",
      "Epoch 0_77.961%:  Training average Loss: 1.067098\n",
      "Epoch 0_79.029%:  Training average Loss: 1.066449\n",
      "Epoch 0_80.097%:  Training average Loss: 1.066016\n",
      "Epoch 0_81.165%:  Training average Loss: 1.065656\n",
      "Epoch 0_82.233%:  Training average Loss: 1.065459\n",
      "Epoch 0_83.301%:  Training average Loss: 1.065749\n",
      "Epoch 0_84.369%:  Training average Loss: 1.065505\n",
      "Epoch 0_85.437%:  Training average Loss: 1.065150\n",
      "Epoch 0_86.505%:  Training average Loss: 1.065198\n",
      "Epoch 0_87.573%:  Training average Loss: 1.065352\n",
      "Epoch 0_88.641%:  Training average Loss: 1.064871\n",
      "Epoch 0_89.709%:  Training average Loss: 1.064646\n",
      "Epoch 0_90.777%:  Training average Loss: 1.064498\n",
      "Epoch 0_91.845%:  Training average Loss: 1.063762\n",
      "Epoch 0_92.913%:  Training average Loss: 1.063408\n",
      "Epoch 0_93.981%:  Training average Loss: 1.062782\n",
      "Epoch 0_95.049%:  Training average Loss: 1.062139\n",
      "Epoch 0_96.117%:  Training average Loss: 1.061974\n",
      "Epoch 0_97.185%:  Training average Loss: 1.061711\n",
      "Epoch 0_98.253%:  Training average Loss: 1.061783\n",
      "Epoch 0_99.321%:  Training average Loss: 1.061953\n",
      "Epoch 0 :  Verification average Loss: 0.990276, Verification accuracy: 59.275279%,Total Time:40.114642\n",
      "Model is saved in model_dict/model_glove/epoch_0_accuracy_0.592753\n",
      "Epoch 1_1.068%:  Training average Loss: 1.037494\n",
      "Epoch 1_2.136%:  Training average Loss: 1.023509\n",
      "Epoch 1_3.204%:  Training average Loss: 1.017861\n",
      "Epoch 1_4.272%:  Training average Loss: 1.019810\n",
      "Epoch 1_5.340%:  Training average Loss: 1.022173\n",
      "Epoch 1_6.408%:  Training average Loss: 1.022376\n",
      "Epoch 1_7.476%:  Training average Loss: 1.017616\n",
      "Epoch 1_8.544%:  Training average Loss: 1.014328\n",
      "Epoch 1_9.612%:  Training average Loss: 1.014756\n",
      "Epoch 1_10.680%:  Training average Loss: 1.015594\n",
      "Epoch 1_11.748%:  Training average Loss: 1.016484\n",
      "Epoch 1_12.816%:  Training average Loss: 1.017004\n",
      "Epoch 1_13.884%:  Training average Loss: 1.017558\n",
      "Epoch 1_14.952%:  Training average Loss: 1.017529\n",
      "Epoch 1_16.019%:  Training average Loss: 1.019923\n",
      "Epoch 1_17.087%:  Training average Loss: 1.019919\n",
      "Epoch 1_18.155%:  Training average Loss: 1.014644\n",
      "Epoch 1_19.223%:  Training average Loss: 1.015178\n",
      "Epoch 1_20.291%:  Training average Loss: 1.014129\n",
      "Epoch 1_21.359%:  Training average Loss: 1.015601\n",
      "Epoch 1_22.427%:  Training average Loss: 1.016162\n",
      "Epoch 1_23.495%:  Training average Loss: 1.018321\n",
      "Epoch 1_24.563%:  Training average Loss: 1.016783\n",
      "Epoch 1_25.631%:  Training average Loss: 1.017206\n",
      "Epoch 1_26.699%:  Training average Loss: 1.017843\n",
      "Epoch 1_27.767%:  Training average Loss: 1.018765\n",
      "Epoch 1_28.835%:  Training average Loss: 1.019018\n",
      "Epoch 1_29.903%:  Training average Loss: 1.018176\n",
      "Epoch 1_30.971%:  Training average Loss: 1.016739\n",
      "Epoch 1_32.039%:  Training average Loss: 1.015783\n",
      "Epoch 1_33.107%:  Training average Loss: 1.015687\n",
      "Epoch 1_34.175%:  Training average Loss: 1.016692\n",
      "Epoch 1_35.243%:  Training average Loss: 1.016467\n",
      "Epoch 1_36.311%:  Training average Loss: 1.016107\n",
      "Epoch 1_37.379%:  Training average Loss: 1.015986\n",
      "Epoch 1_38.447%:  Training average Loss: 1.016608\n",
      "Epoch 1_39.515%:  Training average Loss: 1.017813\n",
      "Epoch 1_40.583%:  Training average Loss: 1.016353\n",
      "Epoch 1_41.651%:  Training average Loss: 1.018023\n",
      "Epoch 1_42.719%:  Training average Loss: 1.017429\n",
      "Epoch 1_43.787%:  Training average Loss: 1.017335\n",
      "Epoch 1_44.855%:  Training average Loss: 1.017806\n",
      "Epoch 1_45.923%:  Training average Loss: 1.017885\n",
      "Epoch 1_46.990%:  Training average Loss: 1.017982\n",
      "Epoch 1_48.058%:  Training average Loss: 1.018584\n",
      "Epoch 1_49.126%:  Training average Loss: 1.018270\n",
      "Epoch 1_50.194%:  Training average Loss: 1.017350\n",
      "Epoch 1_51.262%:  Training average Loss: 1.017431\n",
      "Epoch 1_52.330%:  Training average Loss: 1.017829\n",
      "Epoch 1_53.398%:  Training average Loss: 1.019149\n",
      "Epoch 1_54.466%:  Training average Loss: 1.019418\n",
      "Epoch 1_55.534%:  Training average Loss: 1.019477\n",
      "Epoch 1_56.602%:  Training average Loss: 1.019405\n",
      "Epoch 1_57.670%:  Training average Loss: 1.019852\n",
      "Epoch 1_58.738%:  Training average Loss: 1.020560\n",
      "Epoch 1_59.806%:  Training average Loss: 1.020318\n",
      "Epoch 1_60.874%:  Training average Loss: 1.019977\n",
      "Epoch 1_61.942%:  Training average Loss: 1.020047\n",
      "Epoch 1_63.010%:  Training average Loss: 1.020254\n",
      "Epoch 1_64.078%:  Training average Loss: 1.020249\n",
      "Epoch 1_65.146%:  Training average Loss: 1.020403\n",
      "Epoch 1_66.214%:  Training average Loss: 1.019817\n",
      "Epoch 1_67.282%:  Training average Loss: 1.019676\n",
      "Epoch 1_68.350%:  Training average Loss: 1.019456\n",
      "Epoch 1_69.418%:  Training average Loss: 1.019173\n",
      "Epoch 1_70.486%:  Training average Loss: 1.019194\n",
      "Epoch 1_71.554%:  Training average Loss: 1.019712\n",
      "Epoch 1_72.622%:  Training average Loss: 1.019389\n",
      "Epoch 1_73.690%:  Training average Loss: 1.019857\n",
      "Epoch 1_74.758%:  Training average Loss: 1.019434\n",
      "Epoch 1_75.826%:  Training average Loss: 1.019074\n",
      "Epoch 1_76.894%:  Training average Loss: 1.018772\n",
      "Epoch 1_77.961%:  Training average Loss: 1.018863\n",
      "Epoch 1_79.029%:  Training average Loss: 1.018512\n",
      "Epoch 1_80.097%:  Training average Loss: 1.018363\n",
      "Epoch 1_81.165%:  Training average Loss: 1.017896\n",
      "Epoch 1_82.233%:  Training average Loss: 1.017876\n",
      "Epoch 1_83.301%:  Training average Loss: 1.017804\n",
      "Epoch 1_84.369%:  Training average Loss: 1.017086\n",
      "Epoch 1_85.437%:  Training average Loss: 1.017291\n",
      "Epoch 1_86.505%:  Training average Loss: 1.017735\n",
      "Epoch 1_87.573%:  Training average Loss: 1.018033\n",
      "Epoch 1_88.641%:  Training average Loss: 1.018085\n",
      "Epoch 1_89.709%:  Training average Loss: 1.017718\n",
      "Epoch 1_90.777%:  Training average Loss: 1.017701\n",
      "Epoch 1_91.845%:  Training average Loss: 1.017787\n",
      "Epoch 1_92.913%:  Training average Loss: 1.017430\n",
      "Epoch 1_93.981%:  Training average Loss: 1.017451\n",
      "Epoch 1_95.049%:  Training average Loss: 1.017506\n",
      "Epoch 1_96.117%:  Training average Loss: 1.016797\n",
      "Epoch 1_97.185%:  Training average Loss: 1.016658\n",
      "Epoch 1_98.253%:  Training average Loss: 1.016675\n",
      "Epoch 1_99.321%:  Training average Loss: 1.016821\n",
      "Epoch 1 :  Verification average Loss: 0.993704, Verification accuracy: 58.804306%,Total Time:77.771235\n",
      "Epoch 2_1.068%:  Training average Loss: 0.978408\n",
      "Epoch 2_2.136%:  Training average Loss: 0.986646\n",
      "Epoch 2_3.204%:  Training average Loss: 1.001450\n",
      "Epoch 2_4.272%:  Training average Loss: 0.995391\n",
      "Epoch 2_5.340%:  Training average Loss: 0.990456\n",
      "Epoch 2_6.408%:  Training average Loss: 0.984884\n",
      "Epoch 2_7.476%:  Training average Loss: 0.983758\n",
      "Epoch 2_8.544%:  Training average Loss: 0.987072\n",
      "Epoch 2_9.612%:  Training average Loss: 0.982112\n",
      "Epoch 2_10.680%:  Training average Loss: 0.985148\n",
      "Epoch 2_11.748%:  Training average Loss: 0.984779\n",
      "Epoch 2_12.816%:  Training average Loss: 0.983454\n",
      "Epoch 2_13.884%:  Training average Loss: 0.983647\n",
      "Epoch 2_14.952%:  Training average Loss: 0.983207\n",
      "Epoch 2_16.019%:  Training average Loss: 0.983786\n",
      "Epoch 2_17.087%:  Training average Loss: 0.984217\n",
      "Epoch 2_18.155%:  Training average Loss: 0.983542\n",
      "Epoch 2_19.223%:  Training average Loss: 0.985388\n",
      "Epoch 2_20.291%:  Training average Loss: 0.985901\n",
      "Epoch 2_21.359%:  Training average Loss: 0.986190\n",
      "Epoch 2_22.427%:  Training average Loss: 0.987240\n",
      "Epoch 2_23.495%:  Training average Loss: 0.987116\n",
      "Epoch 2_24.563%:  Training average Loss: 0.988014\n",
      "Epoch 2_25.631%:  Training average Loss: 0.990268\n",
      "Epoch 2_26.699%:  Training average Loss: 0.990710\n",
      "Epoch 2_27.767%:  Training average Loss: 0.990571\n",
      "Epoch 2_28.835%:  Training average Loss: 0.991516\n",
      "Epoch 2_29.903%:  Training average Loss: 0.990762\n",
      "Epoch 2_30.971%:  Training average Loss: 0.992543\n",
      "Epoch 2_32.039%:  Training average Loss: 0.993815\n",
      "Epoch 2_33.107%:  Training average Loss: 0.992062\n",
      "Epoch 2_34.175%:  Training average Loss: 0.991859\n",
      "Epoch 2_35.243%:  Training average Loss: 0.991968\n",
      "Epoch 2_36.311%:  Training average Loss: 0.991645\n",
      "Epoch 2_37.379%:  Training average Loss: 0.991559\n",
      "Epoch 2_38.447%:  Training average Loss: 0.993000\n",
      "Epoch 2_39.515%:  Training average Loss: 0.993465\n",
      "Epoch 2_40.583%:  Training average Loss: 0.993874\n",
      "Epoch 2_41.651%:  Training average Loss: 0.993787\n",
      "Epoch 2_42.719%:  Training average Loss: 0.993705\n",
      "Epoch 2_43.787%:  Training average Loss: 0.994550\n",
      "Epoch 2_44.855%:  Training average Loss: 0.994697\n",
      "Epoch 2_45.923%:  Training average Loss: 0.994399\n",
      "Epoch 2_46.990%:  Training average Loss: 0.993778\n",
      "Epoch 2_48.058%:  Training average Loss: 0.994234\n",
      "Epoch 2_49.126%:  Training average Loss: 0.993896\n",
      "Epoch 2_50.194%:  Training average Loss: 0.993735\n",
      "Epoch 2_51.262%:  Training average Loss: 0.993795\n",
      "Epoch 2_52.330%:  Training average Loss: 0.993367\n",
      "Epoch 2_53.398%:  Training average Loss: 0.993664\n",
      "Epoch 2_54.466%:  Training average Loss: 0.994063\n",
      "Epoch 2_55.534%:  Training average Loss: 0.993770\n",
      "Epoch 2_56.602%:  Training average Loss: 0.994369\n",
      "Epoch 2_57.670%:  Training average Loss: 0.993953\n",
      "Epoch 2_58.738%:  Training average Loss: 0.994528\n",
      "Epoch 2_59.806%:  Training average Loss: 0.995082\n",
      "Epoch 2_60.874%:  Training average Loss: 0.994646\n",
      "Epoch 2_61.942%:  Training average Loss: 0.994357\n",
      "Epoch 2_63.010%:  Training average Loss: 0.993809\n",
      "Epoch 2_64.078%:  Training average Loss: 0.993997\n",
      "Epoch 2_65.146%:  Training average Loss: 0.994512\n",
      "Epoch 2_66.214%:  Training average Loss: 0.995499\n",
      "Epoch 2_67.282%:  Training average Loss: 0.995192\n",
      "Epoch 2_68.350%:  Training average Loss: 0.995355\n",
      "Epoch 2_69.418%:  Training average Loss: 0.995154\n",
      "Epoch 2_70.486%:  Training average Loss: 0.995510\n",
      "Epoch 2_71.554%:  Training average Loss: 0.995882\n",
      "Epoch 2_72.622%:  Training average Loss: 0.995627\n",
      "Epoch 2_73.690%:  Training average Loss: 0.995688\n",
      "Epoch 2_74.758%:  Training average Loss: 0.995771\n",
      "Epoch 2_75.826%:  Training average Loss: 0.995867\n",
      "Epoch 2_76.894%:  Training average Loss: 0.995735\n",
      "Epoch 2_77.961%:  Training average Loss: 0.996098\n",
      "Epoch 2_79.029%:  Training average Loss: 0.996424\n",
      "Epoch 2_80.097%:  Training average Loss: 0.996386\n",
      "Epoch 2_81.165%:  Training average Loss: 0.996478\n",
      "Epoch 2_82.233%:  Training average Loss: 0.996083\n",
      "Epoch 2_83.301%:  Training average Loss: 0.995658\n",
      "Epoch 2_84.369%:  Training average Loss: 0.996007\n",
      "Epoch 2_85.437%:  Training average Loss: 0.995574\n",
      "Epoch 2_86.505%:  Training average Loss: 0.995814\n",
      "Epoch 2_87.573%:  Training average Loss: 0.995532\n",
      "Epoch 2_88.641%:  Training average Loss: 0.995729\n",
      "Epoch 2_89.709%:  Training average Loss: 0.995384\n",
      "Epoch 2_90.777%:  Training average Loss: 0.995858\n",
      "Epoch 2_91.845%:  Training average Loss: 0.995536\n",
      "Epoch 2_92.913%:  Training average Loss: 0.995329\n",
      "Epoch 2_93.981%:  Training average Loss: 0.995747\n",
      "Epoch 2_95.049%:  Training average Loss: 0.996319\n",
      "Epoch 2_96.117%:  Training average Loss: 0.996697\n",
      "Epoch 2_97.185%:  Training average Loss: 0.997117\n",
      "Epoch 2_98.253%:  Training average Loss: 0.996787\n",
      "Epoch 2_99.321%:  Training average Loss: 0.996938\n",
      "Epoch 2 :  Verification average Loss: 0.990508, Verification accuracy: 59.502755%,Total Time:115.964731\n",
      "Model is saved in model_dict/model_glove/epoch_2_accuracy_0.595028\n",
      "Epoch 3_1.068%:  Training average Loss: 0.892757\n",
      "Epoch 3_2.136%:  Training average Loss: 0.946843\n",
      "Epoch 3_3.204%:  Training average Loss: 0.955529\n",
      "Epoch 3_4.272%:  Training average Loss: 0.953561\n",
      "Epoch 3_5.340%:  Training average Loss: 0.962059\n",
      "Epoch 3_6.408%:  Training average Loss: 0.963027\n",
      "Epoch 3_7.476%:  Training average Loss: 0.959015\n",
      "Epoch 3_8.544%:  Training average Loss: 0.959332\n",
      "Epoch 3_9.612%:  Training average Loss: 0.961235\n",
      "Epoch 3_10.680%:  Training average Loss: 0.962737\n",
      "Epoch 3_11.748%:  Training average Loss: 0.965521\n",
      "Epoch 3_12.816%:  Training average Loss: 0.965100\n",
      "Epoch 3_13.884%:  Training average Loss: 0.969030\n",
      "Epoch 3_14.952%:  Training average Loss: 0.970210\n",
      "Epoch 3_16.019%:  Training average Loss: 0.969066\n",
      "Epoch 3_17.087%:  Training average Loss: 0.971168\n",
      "Epoch 3_18.155%:  Training average Loss: 0.971194\n",
      "Epoch 3_19.223%:  Training average Loss: 0.971088\n",
      "Epoch 3_20.291%:  Training average Loss: 0.971831\n",
      "Epoch 3_21.359%:  Training average Loss: 0.970042\n",
      "Epoch 3_22.427%:  Training average Loss: 0.971969\n",
      "Epoch 3_23.495%:  Training average Loss: 0.969984\n",
      "Epoch 3_24.563%:  Training average Loss: 0.972239\n",
      "Epoch 3_25.631%:  Training average Loss: 0.972471\n",
      "Epoch 3_26.699%:  Training average Loss: 0.971741\n",
      "Epoch 3_27.767%:  Training average Loss: 0.972124\n",
      "Epoch 3_28.835%:  Training average Loss: 0.971998\n",
      "Epoch 3_29.903%:  Training average Loss: 0.972908\n",
      "Epoch 3_30.971%:  Training average Loss: 0.973459\n",
      "Epoch 3_32.039%:  Training average Loss: 0.972940\n",
      "Epoch 3_33.107%:  Training average Loss: 0.973882\n",
      "Epoch 3_34.175%:  Training average Loss: 0.974784\n",
      "Epoch 3_35.243%:  Training average Loss: 0.975055\n",
      "Epoch 3_36.311%:  Training average Loss: 0.975247\n",
      "Epoch 3_37.379%:  Training average Loss: 0.974702\n",
      "Epoch 3_38.447%:  Training average Loss: 0.975518\n",
      "Epoch 3_39.515%:  Training average Loss: 0.974783\n",
      "Epoch 3_40.583%:  Training average Loss: 0.975025\n",
      "Epoch 3_41.651%:  Training average Loss: 0.975395\n",
      "Epoch 3_42.719%:  Training average Loss: 0.975381\n",
      "Epoch 3_43.787%:  Training average Loss: 0.975598\n",
      "Epoch 3_44.855%:  Training average Loss: 0.975379\n",
      "Epoch 3_45.923%:  Training average Loss: 0.975970\n",
      "Epoch 3_46.990%:  Training average Loss: 0.976275\n",
      "Epoch 3_48.058%:  Training average Loss: 0.977140\n",
      "Epoch 3_49.126%:  Training average Loss: 0.976112\n",
      "Epoch 3_50.194%:  Training average Loss: 0.975915\n",
      "Epoch 3_51.262%:  Training average Loss: 0.975988\n",
      "Epoch 3_52.330%:  Training average Loss: 0.976181\n",
      "Epoch 3_53.398%:  Training average Loss: 0.977142\n",
      "Epoch 3_54.466%:  Training average Loss: 0.977696\n",
      "Epoch 3_55.534%:  Training average Loss: 0.977570\n",
      "Epoch 3_56.602%:  Training average Loss: 0.977201\n",
      "Epoch 3_57.670%:  Training average Loss: 0.977537\n",
      "Epoch 3_58.738%:  Training average Loss: 0.977885\n",
      "Epoch 3_59.806%:  Training average Loss: 0.977872\n",
      "Epoch 3_60.874%:  Training average Loss: 0.978214\n",
      "Epoch 3_61.942%:  Training average Loss: 0.978827\n",
      "Epoch 3_63.010%:  Training average Loss: 0.978768\n",
      "Epoch 3_64.078%:  Training average Loss: 0.978924\n",
      "Epoch 3_65.146%:  Training average Loss: 0.978947\n",
      "Epoch 3_66.214%:  Training average Loss: 0.979013\n",
      "Epoch 3_67.282%:  Training average Loss: 0.978771\n",
      "Epoch 3_68.350%:  Training average Loss: 0.978587\n",
      "Epoch 3_69.418%:  Training average Loss: 0.979198\n",
      "Epoch 3_70.486%:  Training average Loss: 0.979605\n",
      "Epoch 3_71.554%:  Training average Loss: 0.979605\n",
      "Epoch 3_72.622%:  Training average Loss: 0.979201\n",
      "Epoch 3_73.690%:  Training average Loss: 0.979412\n",
      "Epoch 3_74.758%:  Training average Loss: 0.978975\n",
      "Epoch 3_75.826%:  Training average Loss: 0.979635\n",
      "Epoch 3_76.894%:  Training average Loss: 0.979702\n",
      "Epoch 3_77.961%:  Training average Loss: 0.979953\n",
      "Epoch 3_79.029%:  Training average Loss: 0.979658\n",
      "Epoch 3_80.097%:  Training average Loss: 0.978926\n",
      "Epoch 3_81.165%:  Training average Loss: 0.978711\n",
      "Epoch 3_82.233%:  Training average Loss: 0.979018\n",
      "Epoch 3_83.301%:  Training average Loss: 0.978516\n",
      "Epoch 3_84.369%:  Training average Loss: 0.978501\n",
      "Epoch 3_85.437%:  Training average Loss: 0.978633\n",
      "Epoch 3_86.505%:  Training average Loss: 0.978387\n",
      "Epoch 3_87.573%:  Training average Loss: 0.978739\n",
      "Epoch 3_88.641%:  Training average Loss: 0.979266\n",
      "Epoch 3_89.709%:  Training average Loss: 0.979117\n",
      "Epoch 3_90.777%:  Training average Loss: 0.979531\n",
      "Epoch 3_91.845%:  Training average Loss: 0.979532\n",
      "Epoch 3_92.913%:  Training average Loss: 0.979518\n",
      "Epoch 3_93.981%:  Training average Loss: 0.979185\n",
      "Epoch 3_95.049%:  Training average Loss: 0.979091\n",
      "Epoch 3_96.117%:  Training average Loss: 0.979737\n",
      "Epoch 3_97.185%:  Training average Loss: 0.979684\n",
      "Epoch 3_98.253%:  Training average Loss: 0.979314\n",
      "Epoch 3_99.321%:  Training average Loss: 0.978886\n",
      "Epoch 3 :  Verification average Loss: 0.954187, Verification accuracy: 61.008586%,Total Time:154.376794\n",
      "Model is saved in model_dict/model_glove/epoch_3_accuracy_0.610086\n",
      "Epoch 4_1.068%:  Training average Loss: 0.927202\n",
      "Epoch 4_2.136%:  Training average Loss: 0.958069\n",
      "Epoch 4_3.204%:  Training average Loss: 0.963136\n",
      "Epoch 4_4.272%:  Training average Loss: 0.974383\n",
      "Epoch 4_5.340%:  Training average Loss: 0.973474\n",
      "Epoch 4_6.408%:  Training average Loss: 0.970192\n",
      "Epoch 4_7.476%:  Training average Loss: 0.967687\n",
      "Epoch 4_8.544%:  Training average Loss: 0.968170\n",
      "Epoch 4_9.612%:  Training average Loss: 0.962691\n",
      "Epoch 4_10.680%:  Training average Loss: 0.965730\n",
      "Epoch 4_11.748%:  Training average Loss: 0.963122\n",
      "Epoch 4_12.816%:  Training average Loss: 0.963082\n",
      "Epoch 4_13.884%:  Training average Loss: 0.958594\n",
      "Epoch 4_14.952%:  Training average Loss: 0.959834\n",
      "Epoch 4_16.019%:  Training average Loss: 0.957084\n",
      "Epoch 4_17.087%:  Training average Loss: 0.957334\n",
      "Epoch 4_18.155%:  Training average Loss: 0.959006\n",
      "Epoch 4_19.223%:  Training average Loss: 0.959692\n",
      "Epoch 4_20.291%:  Training average Loss: 0.959753\n",
      "Epoch 4_21.359%:  Training average Loss: 0.959248\n",
      "Epoch 4_22.427%:  Training average Loss: 0.960049\n",
      "Epoch 4_23.495%:  Training average Loss: 0.959598\n",
      "Epoch 4_24.563%:  Training average Loss: 0.963544\n",
      "Epoch 4_25.631%:  Training average Loss: 0.964180\n",
      "Epoch 4_26.699%:  Training average Loss: 0.963126\n",
      "Epoch 4_27.767%:  Training average Loss: 0.962215\n",
      "Epoch 4_28.835%:  Training average Loss: 0.961095\n",
      "Epoch 4_29.903%:  Training average Loss: 0.962174\n",
      "Epoch 4_30.971%:  Training average Loss: 0.960794\n",
      "Epoch 4_32.039%:  Training average Loss: 0.961425\n",
      "Epoch 4_33.107%:  Training average Loss: 0.960534\n",
      "Epoch 4_34.175%:  Training average Loss: 0.961440\n",
      "Epoch 4_35.243%:  Training average Loss: 0.962154\n",
      "Epoch 4_36.311%:  Training average Loss: 0.962548\n",
      "Epoch 4_37.379%:  Training average Loss: 0.961806\n",
      "Epoch 4_38.447%:  Training average Loss: 0.961338\n",
      "Epoch 4_39.515%:  Training average Loss: 0.962030\n",
      "Epoch 4_40.583%:  Training average Loss: 0.963858\n",
      "Epoch 4_41.651%:  Training average Loss: 0.963002\n",
      "Epoch 4_42.719%:  Training average Loss: 0.963416\n",
      "Epoch 4_43.787%:  Training average Loss: 0.962845\n",
      "Epoch 4_44.855%:  Training average Loss: 0.964188\n",
      "Epoch 4_45.923%:  Training average Loss: 0.964050\n",
      "Epoch 4_46.990%:  Training average Loss: 0.965217\n",
      "Epoch 4_48.058%:  Training average Loss: 0.964818\n",
      "Epoch 4_49.126%:  Training average Loss: 0.965138\n",
      "Epoch 4_50.194%:  Training average Loss: 0.964807\n",
      "Epoch 4_51.262%:  Training average Loss: 0.964373\n",
      "Epoch 4_52.330%:  Training average Loss: 0.964234\n",
      "Epoch 4_53.398%:  Training average Loss: 0.964290\n",
      "Epoch 4_54.466%:  Training average Loss: 0.964292\n",
      "Epoch 4_55.534%:  Training average Loss: 0.964783\n",
      "Epoch 4_56.602%:  Training average Loss: 0.964191\n",
      "Epoch 4_57.670%:  Training average Loss: 0.964086\n",
      "Epoch 4_58.738%:  Training average Loss: 0.963972\n",
      "Epoch 4_59.806%:  Training average Loss: 0.964638\n",
      "Epoch 4_60.874%:  Training average Loss: 0.964349\n",
      "Epoch 4_61.942%:  Training average Loss: 0.964420\n",
      "Epoch 4_63.010%:  Training average Loss: 0.964142\n",
      "Epoch 4_64.078%:  Training average Loss: 0.965162\n",
      "Epoch 4_65.146%:  Training average Loss: 0.965090\n",
      "Epoch 4_66.214%:  Training average Loss: 0.964953\n",
      "Epoch 4_67.282%:  Training average Loss: 0.965240\n",
      "Epoch 4_68.350%:  Training average Loss: 0.965752\n",
      "Epoch 4_69.418%:  Training average Loss: 0.966124\n",
      "Epoch 4_70.486%:  Training average Loss: 0.966150\n",
      "Epoch 4_71.554%:  Training average Loss: 0.966465\n",
      "Epoch 4_72.622%:  Training average Loss: 0.966559\n",
      "Epoch 4_73.690%:  Training average Loss: 0.966914\n",
      "Epoch 4_74.758%:  Training average Loss: 0.966296\n",
      "Epoch 4_75.826%:  Training average Loss: 0.966537\n",
      "Epoch 4_76.894%:  Training average Loss: 0.966909\n",
      "Epoch 4_77.961%:  Training average Loss: 0.966340\n",
      "Epoch 4_79.029%:  Training average Loss: 0.966374\n",
      "Epoch 4_80.097%:  Training average Loss: 0.966396\n",
      "Epoch 4_81.165%:  Training average Loss: 0.965821\n",
      "Epoch 4_82.233%:  Training average Loss: 0.965922\n",
      "Epoch 4_83.301%:  Training average Loss: 0.965527\n",
      "Epoch 4_84.369%:  Training average Loss: 0.965970\n",
      "Epoch 4_85.437%:  Training average Loss: 0.966431\n",
      "Epoch 4_86.505%:  Training average Loss: 0.966187\n",
      "Epoch 4_87.573%:  Training average Loss: 0.966108\n",
      "Epoch 4_88.641%:  Training average Loss: 0.967101\n",
      "Epoch 4_89.709%:  Training average Loss: 0.967480\n",
      "Epoch 4_90.777%:  Training average Loss: 0.967507\n",
      "Epoch 4_91.845%:  Training average Loss: 0.967956\n",
      "Epoch 4_92.913%:  Training average Loss: 0.967689\n",
      "Epoch 4_93.981%:  Training average Loss: 0.967824\n",
      "Epoch 4_95.049%:  Training average Loss: 0.968182\n",
      "Epoch 4_96.117%:  Training average Loss: 0.968484\n",
      "Epoch 4_97.185%:  Training average Loss: 0.968651\n",
      "Epoch 4_98.253%:  Training average Loss: 0.968753\n",
      "Epoch 4_99.321%:  Training average Loss: 0.968331\n",
      "Epoch 4 :  Verification average Loss: 0.941587, Verification accuracy: 61.412277%,Total Time:192.226218\n",
      "Model is saved in model_dict/model_glove/epoch_4_accuracy_0.614123\n",
      "Epoch 5_1.068%:  Training average Loss: 0.924370\n",
      "Epoch 5_2.136%:  Training average Loss: 0.962695\n",
      "Epoch 5_3.204%:  Training average Loss: 0.955773\n",
      "Epoch 5_4.272%:  Training average Loss: 0.957248\n",
      "Epoch 5_5.340%:  Training average Loss: 0.960757\n",
      "Epoch 5_6.408%:  Training average Loss: 0.956564\n",
      "Epoch 5_7.476%:  Training average Loss: 0.957771\n",
      "Epoch 5_8.544%:  Training average Loss: 0.957293\n",
      "Epoch 5_9.612%:  Training average Loss: 0.954502\n",
      "Epoch 5_10.680%:  Training average Loss: 0.951982\n",
      "Epoch 5_11.748%:  Training average Loss: 0.952619\n",
      "Epoch 5_12.816%:  Training average Loss: 0.950014\n",
      "Epoch 5_13.884%:  Training average Loss: 0.950755\n",
      "Epoch 5_14.952%:  Training average Loss: 0.952437\n",
      "Epoch 5_16.019%:  Training average Loss: 0.951337\n",
      "Epoch 5_17.087%:  Training average Loss: 0.952247\n",
      "Epoch 5_18.155%:  Training average Loss: 0.953291\n",
      "Epoch 5_19.223%:  Training average Loss: 0.953582\n",
      "Epoch 5_20.291%:  Training average Loss: 0.954305\n",
      "Epoch 5_21.359%:  Training average Loss: 0.952020\n",
      "Epoch 5_22.427%:  Training average Loss: 0.952213\n",
      "Epoch 5_23.495%:  Training average Loss: 0.953287\n",
      "Epoch 5_24.563%:  Training average Loss: 0.953870\n",
      "Epoch 5_25.631%:  Training average Loss: 0.952562\n",
      "Epoch 5_26.699%:  Training average Loss: 0.953962\n",
      "Epoch 5_27.767%:  Training average Loss: 0.954033\n",
      "Epoch 5_28.835%:  Training average Loss: 0.953671\n",
      "Epoch 5_29.903%:  Training average Loss: 0.954683\n",
      "Epoch 5_30.971%:  Training average Loss: 0.953982\n",
      "Epoch 5_32.039%:  Training average Loss: 0.953826\n",
      "Epoch 5_33.107%:  Training average Loss: 0.954180\n",
      "Epoch 5_34.175%:  Training average Loss: 0.955003\n",
      "Epoch 5_35.243%:  Training average Loss: 0.955600\n",
      "Epoch 5_36.311%:  Training average Loss: 0.955700\n",
      "Epoch 5_37.379%:  Training average Loss: 0.955688\n",
      "Epoch 5_38.447%:  Training average Loss: 0.955919\n",
      "Epoch 5_39.515%:  Training average Loss: 0.954943\n",
      "Epoch 5_40.583%:  Training average Loss: 0.956299\n",
      "Epoch 5_41.651%:  Training average Loss: 0.956039\n",
      "Epoch 5_42.719%:  Training average Loss: 0.957523\n",
      "Epoch 5_43.787%:  Training average Loss: 0.957703\n",
      "Epoch 5_44.855%:  Training average Loss: 0.958005\n",
      "Epoch 5_45.923%:  Training average Loss: 0.956950\n",
      "Epoch 5_46.990%:  Training average Loss: 0.957119\n",
      "Epoch 5_48.058%:  Training average Loss: 0.957013\n",
      "Epoch 5_49.126%:  Training average Loss: 0.957214\n",
      "Epoch 5_50.194%:  Training average Loss: 0.958530\n",
      "Epoch 5_51.262%:  Training average Loss: 0.959170\n",
      "Epoch 5_52.330%:  Training average Loss: 0.959254\n",
      "Epoch 5_53.398%:  Training average Loss: 0.959397\n",
      "Epoch 5_54.466%:  Training average Loss: 0.959611\n",
      "Epoch 5_55.534%:  Training average Loss: 0.959174\n",
      "Epoch 5_56.602%:  Training average Loss: 0.958155\n",
      "Epoch 5_57.670%:  Training average Loss: 0.958664\n",
      "Epoch 5_58.738%:  Training average Loss: 0.958484\n",
      "Epoch 5_59.806%:  Training average Loss: 0.959516\n",
      "Epoch 5_60.874%:  Training average Loss: 0.959919\n",
      "Epoch 5_61.942%:  Training average Loss: 0.960568\n",
      "Epoch 5_63.010%:  Training average Loss: 0.960859\n",
      "Epoch 5_64.078%:  Training average Loss: 0.960397\n",
      "Epoch 5_65.146%:  Training average Loss: 0.960027\n",
      "Epoch 5_66.214%:  Training average Loss: 0.960344\n",
      "Epoch 5_67.282%:  Training average Loss: 0.961214\n",
      "Epoch 5_68.350%:  Training average Loss: 0.961258\n",
      "Epoch 5_69.418%:  Training average Loss: 0.961409\n",
      "Epoch 5_70.486%:  Training average Loss: 0.960945\n",
      "Epoch 5_71.554%:  Training average Loss: 0.960924\n",
      "Epoch 5_72.622%:  Training average Loss: 0.960415\n",
      "Epoch 5_73.690%:  Training average Loss: 0.960630\n",
      "Epoch 5_74.758%:  Training average Loss: 0.960627\n",
      "Epoch 5_75.826%:  Training average Loss: 0.960215\n",
      "Epoch 5_76.894%:  Training average Loss: 0.960210\n",
      "Epoch 5_77.961%:  Training average Loss: 0.960477\n",
      "Epoch 5_79.029%:  Training average Loss: 0.960204\n",
      "Epoch 5_80.097%:  Training average Loss: 0.960576\n",
      "Epoch 5_81.165%:  Training average Loss: 0.959873\n",
      "Epoch 5_82.233%:  Training average Loss: 0.960154\n",
      "Epoch 5_83.301%:  Training average Loss: 0.959918\n",
      "Epoch 5_84.369%:  Training average Loss: 0.959691\n",
      "Epoch 5_85.437%:  Training average Loss: 0.960140\n",
      "Epoch 5_86.505%:  Training average Loss: 0.960068\n",
      "Epoch 5_87.573%:  Training average Loss: 0.959731\n",
      "Epoch 5_88.641%:  Training average Loss: 0.959950\n",
      "Epoch 5_89.709%:  Training average Loss: 0.959924\n",
      "Epoch 5_90.777%:  Training average Loss: 0.960160\n",
      "Epoch 5_91.845%:  Training average Loss: 0.959806\n",
      "Epoch 5_92.913%:  Training average Loss: 0.959901\n",
      "Epoch 5_93.981%:  Training average Loss: 0.959842\n",
      "Epoch 5_95.049%:  Training average Loss: 0.959561\n",
      "Epoch 5_96.117%:  Training average Loss: 0.958707\n",
      "Epoch 5_97.185%:  Training average Loss: 0.958188\n",
      "Epoch 5_98.253%:  Training average Loss: 0.958503\n",
      "Epoch 5_99.321%:  Training average Loss: 0.958943\n",
      "Epoch 5 :  Verification average Loss: 0.953572, Verification accuracy: 60.652954%,Total Time:229.627541\n",
      "Epoch 6_1.068%:  Training average Loss: 0.894792\n",
      "Epoch 6_2.136%:  Training average Loss: 0.905721\n",
      "Epoch 6_3.204%:  Training average Loss: 0.907456\n",
      "Epoch 6_4.272%:  Training average Loss: 0.925491\n",
      "Epoch 6_5.340%:  Training average Loss: 0.929241\n",
      "Epoch 6_6.408%:  Training average Loss: 0.931293\n",
      "Epoch 6_7.476%:  Training average Loss: 0.932032\n",
      "Epoch 6_8.544%:  Training average Loss: 0.935214\n",
      "Epoch 6_9.612%:  Training average Loss: 0.935453\n",
      "Epoch 6_10.680%:  Training average Loss: 0.933544\n",
      "Epoch 6_11.748%:  Training average Loss: 0.935155\n",
      "Epoch 6_12.816%:  Training average Loss: 0.937410\n",
      "Epoch 6_13.884%:  Training average Loss: 0.935843\n",
      "Epoch 6_14.952%:  Training average Loss: 0.935749\n",
      "Epoch 6_16.019%:  Training average Loss: 0.936686\n",
      "Epoch 6_17.087%:  Training average Loss: 0.935611\n",
      "Epoch 6_18.155%:  Training average Loss: 0.936680\n",
      "Epoch 6_19.223%:  Training average Loss: 0.936786\n",
      "Epoch 6_20.291%:  Training average Loss: 0.934433\n",
      "Epoch 6_21.359%:  Training average Loss: 0.935907\n",
      "Epoch 6_22.427%:  Training average Loss: 0.936510\n",
      "Epoch 6_23.495%:  Training average Loss: 0.937220\n",
      "Epoch 6_24.563%:  Training average Loss: 0.936445\n",
      "Epoch 6_25.631%:  Training average Loss: 0.936841\n",
      "Epoch 6_26.699%:  Training average Loss: 0.934873\n",
      "Epoch 6_27.767%:  Training average Loss: 0.935546\n",
      "Epoch 6_28.835%:  Training average Loss: 0.934684\n",
      "Epoch 6_29.903%:  Training average Loss: 0.934450\n",
      "Epoch 6_30.971%:  Training average Loss: 0.934706\n",
      "Epoch 6_32.039%:  Training average Loss: 0.933313\n",
      "Epoch 6_33.107%:  Training average Loss: 0.934298\n",
      "Epoch 6_34.175%:  Training average Loss: 0.934161\n",
      "Epoch 6_35.243%:  Training average Loss: 0.934165\n",
      "Epoch 6_36.311%:  Training average Loss: 0.935326\n",
      "Epoch 6_37.379%:  Training average Loss: 0.936786\n",
      "Epoch 6_38.447%:  Training average Loss: 0.936994\n",
      "Epoch 6_39.515%:  Training average Loss: 0.937597\n",
      "Epoch 6_40.583%:  Training average Loss: 0.938723\n",
      "Epoch 6_41.651%:  Training average Loss: 0.938988\n",
      "Epoch 6_42.719%:  Training average Loss: 0.939184\n",
      "Epoch 6_43.787%:  Training average Loss: 0.939876\n",
      "Epoch 6_44.855%:  Training average Loss: 0.939651\n",
      "Epoch 6_45.923%:  Training average Loss: 0.940428\n",
      "Epoch 6_46.990%:  Training average Loss: 0.939951\n",
      "Epoch 6_48.058%:  Training average Loss: 0.940245\n",
      "Epoch 6_49.126%:  Training average Loss: 0.940583\n",
      "Epoch 6_50.194%:  Training average Loss: 0.941007\n",
      "Epoch 6_51.262%:  Training average Loss: 0.940943\n",
      "Epoch 6_52.330%:  Training average Loss: 0.941473\n",
      "Epoch 6_53.398%:  Training average Loss: 0.941133\n",
      "Epoch 6_54.466%:  Training average Loss: 0.942112\n",
      "Epoch 6_55.534%:  Training average Loss: 0.941862\n",
      "Epoch 6_56.602%:  Training average Loss: 0.942204\n",
      "Epoch 6_57.670%:  Training average Loss: 0.942073\n",
      "Epoch 6_58.738%:  Training average Loss: 0.942423\n",
      "Epoch 6_59.806%:  Training average Loss: 0.941805\n",
      "Epoch 6_60.874%:  Training average Loss: 0.942057\n",
      "Epoch 6_61.942%:  Training average Loss: 0.942248\n",
      "Epoch 6_63.010%:  Training average Loss: 0.941835\n",
      "Epoch 6_64.078%:  Training average Loss: 0.941597\n",
      "Epoch 6_65.146%:  Training average Loss: 0.941531\n",
      "Epoch 6_66.214%:  Training average Loss: 0.941257\n",
      "Epoch 6_67.282%:  Training average Loss: 0.941574\n",
      "Epoch 6_68.350%:  Training average Loss: 0.941965\n",
      "Epoch 6_69.418%:  Training average Loss: 0.942144\n",
      "Epoch 6_70.486%:  Training average Loss: 0.942548\n",
      "Epoch 6_71.554%:  Training average Loss: 0.942646\n",
      "Epoch 6_72.622%:  Training average Loss: 0.942712\n",
      "Epoch 6_73.690%:  Training average Loss: 0.942337\n",
      "Epoch 6_74.758%:  Training average Loss: 0.943052\n",
      "Epoch 6_75.826%:  Training average Loss: 0.943545\n",
      "Epoch 6_76.894%:  Training average Loss: 0.943956\n",
      "Epoch 6_77.961%:  Training average Loss: 0.944068\n",
      "Epoch 6_79.029%:  Training average Loss: 0.944321\n",
      "Epoch 6_80.097%:  Training average Loss: 0.945315\n",
      "Epoch 6_81.165%:  Training average Loss: 0.945065\n",
      "Epoch 6_82.233%:  Training average Loss: 0.945836\n",
      "Epoch 6_83.301%:  Training average Loss: 0.945964\n",
      "Epoch 6_84.369%:  Training average Loss: 0.946028\n",
      "Epoch 6_85.437%:  Training average Loss: 0.946592\n",
      "Epoch 6_86.505%:  Training average Loss: 0.947264\n",
      "Epoch 6_87.573%:  Training average Loss: 0.947883\n",
      "Epoch 6_88.641%:  Training average Loss: 0.948189\n",
      "Epoch 6_89.709%:  Training average Loss: 0.948473\n",
      "Epoch 6_90.777%:  Training average Loss: 0.948623\n",
      "Epoch 6_91.845%:  Training average Loss: 0.949541\n",
      "Epoch 6_92.913%:  Training average Loss: 0.949520\n",
      "Epoch 6_93.981%:  Training average Loss: 0.949453\n",
      "Epoch 6_95.049%:  Training average Loss: 0.949433\n",
      "Epoch 6_96.117%:  Training average Loss: 0.949621\n",
      "Epoch 6_97.185%:  Training average Loss: 0.949499\n",
      "Epoch 6_98.253%:  Training average Loss: 0.949116\n",
      "Epoch 6_99.321%:  Training average Loss: 0.949315\n",
      "Epoch 6 :  Verification average Loss: 0.940925, Verification accuracy: 61.514802%,Total Time:267.905830\n",
      "Model is saved in model_dict/model_glove/epoch_6_accuracy_0.615148\n",
      "Epoch 7_1.068%:  Training average Loss: 0.927868\n",
      "Epoch 7_2.136%:  Training average Loss: 0.906225\n",
      "Epoch 7_3.204%:  Training average Loss: 0.901602\n",
      "Epoch 7_4.272%:  Training average Loss: 0.899362\n",
      "Epoch 7_5.340%:  Training average Loss: 0.901892\n",
      "Epoch 7_6.408%:  Training average Loss: 0.903179\n",
      "Epoch 7_7.476%:  Training average Loss: 0.911524\n",
      "Epoch 7_8.544%:  Training average Loss: 0.916402\n",
      "Epoch 7_9.612%:  Training average Loss: 0.916966\n",
      "Epoch 7_10.680%:  Training average Loss: 0.916740\n",
      "Epoch 7_11.748%:  Training average Loss: 0.916739\n",
      "Epoch 7_12.816%:  Training average Loss: 0.917548\n",
      "Epoch 7_13.884%:  Training average Loss: 0.918120\n",
      "Epoch 7_14.952%:  Training average Loss: 0.917423\n",
      "Epoch 7_16.019%:  Training average Loss: 0.918851\n",
      "Epoch 7_17.087%:  Training average Loss: 0.920177\n",
      "Epoch 7_18.155%:  Training average Loss: 0.922727\n",
      "Epoch 7_19.223%:  Training average Loss: 0.923520\n",
      "Epoch 7_20.291%:  Training average Loss: 0.923170\n",
      "Epoch 7_21.359%:  Training average Loss: 0.924688\n",
      "Epoch 7_22.427%:  Training average Loss: 0.926558\n",
      "Epoch 7_23.495%:  Training average Loss: 0.925198\n",
      "Epoch 7_24.563%:  Training average Loss: 0.926333\n",
      "Epoch 7_25.631%:  Training average Loss: 0.925353\n",
      "Epoch 7_26.699%:  Training average Loss: 0.926454\n",
      "Epoch 7_27.767%:  Training average Loss: 0.927659\n",
      "Epoch 7_28.835%:  Training average Loss: 0.927072\n",
      "Epoch 7_29.903%:  Training average Loss: 0.926855\n",
      "Epoch 7_30.971%:  Training average Loss: 0.928180\n",
      "Epoch 7_32.039%:  Training average Loss: 0.929045\n",
      "Epoch 7_33.107%:  Training average Loss: 0.929421\n",
      "Epoch 7_34.175%:  Training average Loss: 0.932534\n",
      "Epoch 7_35.243%:  Training average Loss: 0.934132\n",
      "Epoch 7_36.311%:  Training average Loss: 0.934770\n",
      "Epoch 7_37.379%:  Training average Loss: 0.934655\n",
      "Epoch 7_38.447%:  Training average Loss: 0.935462\n",
      "Epoch 7_39.515%:  Training average Loss: 0.935941\n",
      "Epoch 7_40.583%:  Training average Loss: 0.936800\n",
      "Epoch 7_41.651%:  Training average Loss: 0.936956\n",
      "Epoch 7_42.719%:  Training average Loss: 0.936674\n",
      "Epoch 7_43.787%:  Training average Loss: 0.937230\n",
      "Epoch 7_44.855%:  Training average Loss: 0.936293\n",
      "Epoch 7_45.923%:  Training average Loss: 0.935985\n",
      "Epoch 7_46.990%:  Training average Loss: 0.936623\n",
      "Epoch 7_48.058%:  Training average Loss: 0.936740\n",
      "Epoch 7_49.126%:  Training average Loss: 0.935929\n",
      "Epoch 7_50.194%:  Training average Loss: 0.936699\n",
      "Epoch 7_51.262%:  Training average Loss: 0.937274\n",
      "Epoch 7_52.330%:  Training average Loss: 0.936706\n",
      "Epoch 7_53.398%:  Training average Loss: 0.937348\n",
      "Epoch 7_54.466%:  Training average Loss: 0.937428\n",
      "Epoch 7_55.534%:  Training average Loss: 0.938345\n",
      "Epoch 7_56.602%:  Training average Loss: 0.939161\n",
      "Epoch 7_57.670%:  Training average Loss: 0.939328\n",
      "Epoch 7_58.738%:  Training average Loss: 0.939119\n",
      "Epoch 7_59.806%:  Training average Loss: 0.938336\n",
      "Epoch 7_60.874%:  Training average Loss: 0.938385\n",
      "Epoch 7_61.942%:  Training average Loss: 0.938390\n",
      "Epoch 7_63.010%:  Training average Loss: 0.938137\n",
      "Epoch 7_64.078%:  Training average Loss: 0.937850\n",
      "Epoch 7_65.146%:  Training average Loss: 0.937896\n",
      "Epoch 7_66.214%:  Training average Loss: 0.937682\n",
      "Epoch 7_67.282%:  Training average Loss: 0.937439\n",
      "Epoch 7_68.350%:  Training average Loss: 0.937862\n",
      "Epoch 7_69.418%:  Training average Loss: 0.937784\n",
      "Epoch 7_70.486%:  Training average Loss: 0.939341\n",
      "Epoch 7_71.554%:  Training average Loss: 0.938237\n",
      "Epoch 7_72.622%:  Training average Loss: 0.938812\n",
      "Epoch 7_73.690%:  Training average Loss: 0.938259\n",
      "Epoch 7_74.758%:  Training average Loss: 0.938961\n",
      "Epoch 7_75.826%:  Training average Loss: 0.939442\n",
      "Epoch 7_76.894%:  Training average Loss: 0.939096\n",
      "Epoch 7_77.961%:  Training average Loss: 0.938646\n",
      "Epoch 7_79.029%:  Training average Loss: 0.939522\n",
      "Epoch 7_80.097%:  Training average Loss: 0.939406\n",
      "Epoch 7_81.165%:  Training average Loss: 0.939343\n",
      "Epoch 7_82.233%:  Training average Loss: 0.938912\n",
      "Epoch 7_83.301%:  Training average Loss: 0.939554\n",
      "Epoch 7_84.369%:  Training average Loss: 0.939380\n",
      "Epoch 7_85.437%:  Training average Loss: 0.939247\n",
      "Epoch 7_86.505%:  Training average Loss: 0.939576\n",
      "Epoch 7_87.573%:  Training average Loss: 0.939727\n",
      "Epoch 7_88.641%:  Training average Loss: 0.939690\n",
      "Epoch 7_89.709%:  Training average Loss: 0.940125\n",
      "Epoch 7_90.777%:  Training average Loss: 0.940315\n",
      "Epoch 7_91.845%:  Training average Loss: 0.940459\n",
      "Epoch 7_92.913%:  Training average Loss: 0.940827\n",
      "Epoch 7_93.981%:  Training average Loss: 0.941010\n",
      "Epoch 7_95.049%:  Training average Loss: 0.940777\n",
      "Epoch 7_96.117%:  Training average Loss: 0.941305\n",
      "Epoch 7_97.185%:  Training average Loss: 0.941525\n",
      "Epoch 7_98.253%:  Training average Loss: 0.941836\n",
      "Epoch 7_99.321%:  Training average Loss: 0.941657\n",
      "Epoch 7 :  Verification average Loss: 0.940444, Verification accuracy: 61.562860%,Total Time:305.661053\n",
      "Model is saved in model_dict/model_glove/epoch_7_accuracy_0.615629\n",
      "Epoch 8_1.068%:  Training average Loss: 0.905795\n",
      "Epoch 8_2.136%:  Training average Loss: 0.921466\n",
      "Epoch 8_3.204%:  Training average Loss: 0.908581\n",
      "Epoch 8_4.272%:  Training average Loss: 0.914439\n",
      "Epoch 8_5.340%:  Training average Loss: 0.921697\n",
      "Epoch 8_6.408%:  Training average Loss: 0.924568\n",
      "Epoch 8_7.476%:  Training average Loss: 0.924440\n",
      "Epoch 8_8.544%:  Training average Loss: 0.926420\n",
      "Epoch 8_9.612%:  Training average Loss: 0.926205\n",
      "Epoch 8_10.680%:  Training average Loss: 0.924682\n",
      "Epoch 8_11.748%:  Training average Loss: 0.923733\n",
      "Epoch 8_12.816%:  Training average Loss: 0.923748\n",
      "Epoch 8_13.884%:  Training average Loss: 0.923262\n",
      "Epoch 8_14.952%:  Training average Loss: 0.925291\n",
      "Epoch 8_16.019%:  Training average Loss: 0.924610\n",
      "Epoch 8_17.087%:  Training average Loss: 0.927985\n",
      "Epoch 8_18.155%:  Training average Loss: 0.928244\n",
      "Epoch 8_19.223%:  Training average Loss: 0.927076\n",
      "Epoch 8_20.291%:  Training average Loss: 0.927044\n",
      "Epoch 8_21.359%:  Training average Loss: 0.925618\n",
      "Epoch 8_22.427%:  Training average Loss: 0.924323\n",
      "Epoch 8_23.495%:  Training average Loss: 0.925027\n",
      "Epoch 8_24.563%:  Training average Loss: 0.926293\n",
      "Epoch 8_25.631%:  Training average Loss: 0.924433\n",
      "Epoch 8_26.699%:  Training average Loss: 0.926214\n",
      "Epoch 8_27.767%:  Training average Loss: 0.926220\n",
      "Epoch 8_28.835%:  Training average Loss: 0.924367\n",
      "Epoch 8_29.903%:  Training average Loss: 0.924597\n",
      "Epoch 8_30.971%:  Training average Loss: 0.924895\n",
      "Epoch 8_32.039%:  Training average Loss: 0.925621\n",
      "Epoch 8_33.107%:  Training average Loss: 0.925555\n",
      "Epoch 8_34.175%:  Training average Loss: 0.923794\n",
      "Epoch 8_35.243%:  Training average Loss: 0.925527\n",
      "Epoch 8_36.311%:  Training average Loss: 0.926587\n",
      "Epoch 8_37.379%:  Training average Loss: 0.927073\n",
      "Epoch 8_38.447%:  Training average Loss: 0.928401\n",
      "Epoch 8_39.515%:  Training average Loss: 0.928349\n",
      "Epoch 8_40.583%:  Training average Loss: 0.928677\n",
      "Epoch 8_41.651%:  Training average Loss: 0.929212\n",
      "Epoch 8_42.719%:  Training average Loss: 0.928804\n",
      "Epoch 8_43.787%:  Training average Loss: 0.928873\n",
      "Epoch 8_44.855%:  Training average Loss: 0.928205\n",
      "Epoch 8_45.923%:  Training average Loss: 0.928690\n",
      "Epoch 8_46.990%:  Training average Loss: 0.928780\n",
      "Epoch 8_48.058%:  Training average Loss: 0.929195\n",
      "Epoch 8_49.126%:  Training average Loss: 0.929100\n",
      "Epoch 8_50.194%:  Training average Loss: 0.928377\n",
      "Epoch 8_51.262%:  Training average Loss: 0.928219\n",
      "Epoch 8_52.330%:  Training average Loss: 0.927842\n",
      "Epoch 8_53.398%:  Training average Loss: 0.927378\n",
      "Epoch 8_54.466%:  Training average Loss: 0.928796\n",
      "Epoch 8_55.534%:  Training average Loss: 0.928680\n",
      "Epoch 8_56.602%:  Training average Loss: 0.928590\n",
      "Epoch 8_57.670%:  Training average Loss: 0.929161\n",
      "Epoch 8_58.738%:  Training average Loss: 0.929330\n",
      "Epoch 8_59.806%:  Training average Loss: 0.928940\n",
      "Epoch 8_60.874%:  Training average Loss: 0.928590\n",
      "Epoch 8_61.942%:  Training average Loss: 0.929110\n",
      "Epoch 8_63.010%:  Training average Loss: 0.928346\n",
      "Epoch 8_64.078%:  Training average Loss: 0.928761\n",
      "Epoch 8_65.146%:  Training average Loss: 0.929378\n",
      "Epoch 8_66.214%:  Training average Loss: 0.929165\n",
      "Epoch 8_67.282%:  Training average Loss: 0.929535\n",
      "Epoch 8_68.350%:  Training average Loss: 0.930348\n",
      "Epoch 8_69.418%:  Training average Loss: 0.930099\n",
      "Epoch 8_70.486%:  Training average Loss: 0.931205\n",
      "Epoch 8_71.554%:  Training average Loss: 0.931251\n",
      "Epoch 8_72.622%:  Training average Loss: 0.931757\n",
      "Epoch 8_73.690%:  Training average Loss: 0.931883\n",
      "Epoch 8_74.758%:  Training average Loss: 0.931829\n",
      "Epoch 8_75.826%:  Training average Loss: 0.932032\n",
      "Epoch 8_76.894%:  Training average Loss: 0.931791\n",
      "Epoch 8_77.961%:  Training average Loss: 0.932393\n",
      "Epoch 8_79.029%:  Training average Loss: 0.932877\n",
      "Epoch 8_80.097%:  Training average Loss: 0.933119\n",
      "Epoch 8_81.165%:  Training average Loss: 0.932947\n",
      "Epoch 8_82.233%:  Training average Loss: 0.933050\n",
      "Epoch 8_83.301%:  Training average Loss: 0.933124\n",
      "Epoch 8_84.369%:  Training average Loss: 0.933710\n",
      "Epoch 8_85.437%:  Training average Loss: 0.934587\n",
      "Epoch 8_86.505%:  Training average Loss: 0.934399\n",
      "Epoch 8_87.573%:  Training average Loss: 0.934477\n",
      "Epoch 8_88.641%:  Training average Loss: 0.934540\n",
      "Epoch 8_89.709%:  Training average Loss: 0.935358\n",
      "Epoch 8_90.777%:  Training average Loss: 0.935615\n",
      "Epoch 8_91.845%:  Training average Loss: 0.935488\n",
      "Epoch 8_92.913%:  Training average Loss: 0.935216\n",
      "Epoch 8_93.981%:  Training average Loss: 0.935491\n",
      "Epoch 8_95.049%:  Training average Loss: 0.935244\n",
      "Epoch 8_96.117%:  Training average Loss: 0.935397\n",
      "Epoch 8_97.185%:  Training average Loss: 0.935794\n",
      "Epoch 8_98.253%:  Training average Loss: 0.936532\n",
      "Epoch 8_99.321%:  Training average Loss: 0.936268\n",
      "Epoch 8 :  Verification average Loss: 0.934824, Verification accuracy: 62.376650%,Total Time:343.090474\n",
      "Model is saved in model_dict/model_glove/epoch_8_accuracy_0.623767\n",
      "Epoch 9_1.068%:  Training average Loss: 0.910389\n",
      "Epoch 9_2.136%:  Training average Loss: 0.909701\n",
      "Epoch 9_3.204%:  Training average Loss: 0.919167\n",
      "Epoch 9_4.272%:  Training average Loss: 0.921736\n",
      "Epoch 9_5.340%:  Training average Loss: 0.922074\n",
      "Epoch 9_6.408%:  Training average Loss: 0.921080\n",
      "Epoch 9_7.476%:  Training average Loss: 0.922574\n",
      "Epoch 9_8.544%:  Training average Loss: 0.920942\n",
      "Epoch 9_9.612%:  Training average Loss: 0.916691\n",
      "Epoch 9_10.680%:  Training average Loss: 0.913898\n",
      "Epoch 9_11.748%:  Training average Loss: 0.916346\n",
      "Epoch 9_12.816%:  Training average Loss: 0.919741\n",
      "Epoch 9_13.884%:  Training average Loss: 0.922304\n",
      "Epoch 9_14.952%:  Training average Loss: 0.923228\n",
      "Epoch 9_16.019%:  Training average Loss: 0.922078\n",
      "Epoch 9_17.087%:  Training average Loss: 0.921860\n",
      "Epoch 9_18.155%:  Training average Loss: 0.920561\n",
      "Epoch 9_19.223%:  Training average Loss: 0.920602\n",
      "Epoch 9_20.291%:  Training average Loss: 0.920532\n",
      "Epoch 9_21.359%:  Training average Loss: 0.921373\n",
      "Epoch 9_22.427%:  Training average Loss: 0.921765\n",
      "Epoch 9_23.495%:  Training average Loss: 0.922495\n",
      "Epoch 9_24.563%:  Training average Loss: 0.922454\n",
      "Epoch 9_25.631%:  Training average Loss: 0.922023\n",
      "Epoch 9_26.699%:  Training average Loss: 0.920947\n",
      "Epoch 9_27.767%:  Training average Loss: 0.920491\n",
      "Epoch 9_28.835%:  Training average Loss: 0.920038\n",
      "Epoch 9_29.903%:  Training average Loss: 0.920087\n",
      "Epoch 9_30.971%:  Training average Loss: 0.920610\n",
      "Epoch 9_32.039%:  Training average Loss: 0.922685\n",
      "Epoch 9_33.107%:  Training average Loss: 0.920869\n",
      "Epoch 9_34.175%:  Training average Loss: 0.920659\n",
      "Epoch 9_35.243%:  Training average Loss: 0.920997\n",
      "Epoch 9_36.311%:  Training average Loss: 0.921416\n",
      "Epoch 9_37.379%:  Training average Loss: 0.921364\n",
      "Epoch 9_38.447%:  Training average Loss: 0.921442\n",
      "Epoch 9_39.515%:  Training average Loss: 0.921920\n",
      "Epoch 9_40.583%:  Training average Loss: 0.921509\n",
      "Epoch 9_41.651%:  Training average Loss: 0.921141\n",
      "Epoch 9_42.719%:  Training average Loss: 0.920531\n",
      "Epoch 9_43.787%:  Training average Loss: 0.921066\n",
      "Epoch 9_44.855%:  Training average Loss: 0.920849\n",
      "Epoch 9_45.923%:  Training average Loss: 0.921849\n",
      "Epoch 9_46.990%:  Training average Loss: 0.922125\n",
      "Epoch 9_48.058%:  Training average Loss: 0.924241\n",
      "Epoch 9_49.126%:  Training average Loss: 0.924544\n",
      "Epoch 9_50.194%:  Training average Loss: 0.924762\n",
      "Epoch 9_51.262%:  Training average Loss: 0.925143\n",
      "Epoch 9_52.330%:  Training average Loss: 0.925777\n",
      "Epoch 9_53.398%:  Training average Loss: 0.926170\n",
      "Epoch 9_54.466%:  Training average Loss: 0.926272\n",
      "Epoch 9_55.534%:  Training average Loss: 0.926441\n",
      "Epoch 9_56.602%:  Training average Loss: 0.926150\n",
      "Epoch 9_57.670%:  Training average Loss: 0.926582\n",
      "Epoch 9_58.738%:  Training average Loss: 0.926095\n",
      "Epoch 9_59.806%:  Training average Loss: 0.926213\n",
      "Epoch 9_60.874%:  Training average Loss: 0.926042\n",
      "Epoch 9_61.942%:  Training average Loss: 0.926554\n",
      "Epoch 9_63.010%:  Training average Loss: 0.926162\n",
      "Epoch 9_64.078%:  Training average Loss: 0.926504\n",
      "Epoch 9_65.146%:  Training average Loss: 0.926873\n",
      "Epoch 9_66.214%:  Training average Loss: 0.926546\n",
      "Epoch 9_67.282%:  Training average Loss: 0.927092\n",
      "Epoch 9_68.350%:  Training average Loss: 0.927529\n",
      "Epoch 9_69.418%:  Training average Loss: 0.927727\n",
      "Epoch 9_70.486%:  Training average Loss: 0.928005\n",
      "Epoch 9_71.554%:  Training average Loss: 0.926757\n",
      "Epoch 9_72.622%:  Training average Loss: 0.927334\n",
      "Epoch 9_73.690%:  Training average Loss: 0.927334\n",
      "Epoch 9_74.758%:  Training average Loss: 0.926863\n",
      "Epoch 9_75.826%:  Training average Loss: 0.927029\n",
      "Epoch 9_76.894%:  Training average Loss: 0.926567\n",
      "Epoch 9_77.961%:  Training average Loss: 0.926918\n",
      "Epoch 9_79.029%:  Training average Loss: 0.926776\n",
      "Epoch 9_80.097%:  Training average Loss: 0.927014\n",
      "Epoch 9_81.165%:  Training average Loss: 0.926726\n",
      "Epoch 9_82.233%:  Training average Loss: 0.927078\n",
      "Epoch 9_83.301%:  Training average Loss: 0.927472\n",
      "Epoch 9_84.369%:  Training average Loss: 0.928025\n",
      "Epoch 9_85.437%:  Training average Loss: 0.928513\n",
      "Epoch 9_86.505%:  Training average Loss: 0.928472\n",
      "Epoch 9_87.573%:  Training average Loss: 0.928941\n",
      "Epoch 9_88.641%:  Training average Loss: 0.928389\n",
      "Epoch 9_89.709%:  Training average Loss: 0.928915\n",
      "Epoch 9_90.777%:  Training average Loss: 0.929052\n",
      "Epoch 9_91.845%:  Training average Loss: 0.928767\n",
      "Epoch 9_92.913%:  Training average Loss: 0.929178\n",
      "Epoch 9_93.981%:  Training average Loss: 0.929164\n",
      "Epoch 9_95.049%:  Training average Loss: 0.929571\n",
      "Epoch 9_96.117%:  Training average Loss: 0.929652\n",
      "Epoch 9_97.185%:  Training average Loss: 0.929516\n",
      "Epoch 9_98.253%:  Training average Loss: 0.929988\n",
      "Epoch 9_99.321%:  Training average Loss: 0.930319\n",
      "Epoch 9 :  Verification average Loss: 0.943684, Verification accuracy: 62.158785%,Total Time:380.674199\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epoch = 10\n",
    "best_accuracy = 0.0\n",
    "start_time = time.time()\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        steps+=1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_text = batch.Phrase\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch_text)\n",
    "        loss = criterion(out,batch_label)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct = (torch.max(out,dim=1)[1].view(batch_label.size())==batch_label).sum()\n",
    "        total_correct+=correct.item()\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                      %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps))\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(dev_iterator.dataset)\n",
    "    steps = 0.0    \n",
    "    for batch in dev_iterator:\n",
    "        steps+=1\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "        \n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "          %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  \n",
    "        \n",
    "        if best_accuracy < total_correct/total_data_num :\n",
    "            best_accuracy =total_correct/total_data_num \n",
    "            torch.save(model,'model_dict/model_glove/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            print('Model is saved in model_dict/model_glove/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average Loss: 0.930605, Test accuracy: 0.207196，Total time: 2.865470\n"
     ]
    }
   ],
   "source": [
    "PATH='model_dict/model_glove/epoch_8_accuracy_0.623767'\n",
    "model = torch.load(PATH)\n",
    "total_loss=0.0\n",
    "accuracy=0.0\n",
    "total_correct=0.0\n",
    "total_data_num = len(train_iterator.dataset)\n",
    "steps = 0.0    \n",
    "start_time=time.time()\n",
    "for batch in test_iterator:\n",
    "    steps+=1\n",
    "    batch_text=batch.Phrase\n",
    "    batch_label=batch.Sentiment\n",
    "    out=model(batch_text)\n",
    "    loss = criterion(out, batch_label)\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "    total_correct = total_correct + correct.item()\n",
    "    #break   \n",
    "\n",
    "print(\"Test average Loss: %f, Test accuracy: %f，Total time: %f\"\n",
    "  %(total_loss/steps, total_correct/total_data_num,time.time()-start_time) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='model_dict/model_glove/epoch_8_accuracy_0.623767'\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicts=[]\n",
    "    for batch in pred_iterator:\n",
    "        batch_text=batch.Phrase\n",
    "        out=model(batch_text)\n",
    "        predicts.extend(out.argmax(1).cpu().numpy())\n",
    "    \n",
    "    test_data=pd.read_csv(\"test.tsv\",sep='\\t')\n",
    "    test_data[\"Sentiment\"]=predicts\n",
    "    test_data[['PhraseId','Sentiment']].set_index('PhraseId').to_csv('cnn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66293"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
