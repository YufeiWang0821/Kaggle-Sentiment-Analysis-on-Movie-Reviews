{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "BATCH_SIZE=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 数据提取和划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 156057, 156058, 156059])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv(\"train.tsv\",sep='\\t')\n",
    "idx=np.arange(train_data.shape[0])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv(\"test.tsv\",sep='\\t')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集、开发集和测试集的大小\n",
    "np.random.shuffle(idx)\n",
    "train_size=int(len(idx)*0.6)\n",
    "test_size=int(len(idx)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将原数据集划分，生成各自的文件\n",
    "train_data.iloc[idx[:train_size], :].to_csv('data/cnn_train.csv',index=False)\n",
    "train_data.iloc[idx[train_size:test_size], :].to_csv(\"data/cnn_test.csv\", index=False)\n",
    "train_data.iloc[idx[test_size:], :].to_csv(\"data/cnn_dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"data/cnn_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext加载数据\n",
    "from torchtext import data\n",
    "TEXT = data.Field(sequential=True,batch_first=True,lower=True)\n",
    "LABEL =data.Field(sequential=False,batch_first=True,unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "datafields = [# 不需要的filed设置为None\n",
    "    (\"PhraseId\", None), \n",
    "    (\"SentenceId\", None),\n",
    "    ('Phrase', TEXT),\n",
    "    ('Sentiment', LABEL)\n",
    "]\n",
    "datafields2 = [# 不需要的filed设置为None\n",
    "    (\"PhraseId\", None), \n",
    "    (\"SentenceId\", None),\n",
    "    ('Phrase', TEXT),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TabularDataset(path='data/cnn_train.csv',format='csv',fields=datafields,skip_header=True)\n",
    "dev_data=data.TabularDataset(path='data/cnn_dev.csv',format='csv',fields=datafields,skip_header=True)\n",
    "test_data=data.TabularDataset(path='data/cnn_test.csv',format='csv',fields=datafields,skip_header=True)\n",
    "pred=data.TabularDataset(path='data/cnn_pred.csv',format='csv',fields=datafields2,skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,vectors='glove.6B.50d',unk_init= lambda x:torch.nn.init.uniform_(x, a=-0.25, b=0.25))\n",
    "LABEL.build_vocab(train_data)\n",
    "PAD_INDEX = TEXT.vocab.stoi['<pad>']\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代器\n",
    "train_iterator = data.BucketIterator(train_data,batch_size=BATCH_SIZE,train=True,shuffle=True)\n",
    "dev_iterator = data.BucketIterator(dev_data,batch_size=len(dev_data),train=False,sort=False)\n",
    "test_iterator = data.BucketIterator(test_data,batch_size=len(test_data),train=False,sort=False)\n",
    "pred_iterator = data.BucketIterator(pred,batch_size=len(pred),train=False,sort=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16464, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置参数\n",
    "embedding_choice='glove'\n",
    "num_embeddings=len(TEXT.vocab)\n",
    "embedding_dim=50\n",
    "dropoutp=0.5\n",
    "hidden_size=50  #隐藏单元数\n",
    "num_layers=2  #层数\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "vocab_size,label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN实现\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.embedding_choice=embedding_choice        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, \n",
    "            padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
    "            batch_first=True,dropout=dropoutp,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropoutp)    \n",
    "        self.fc = nn.Linear(hidden_size * 2, label_num)\n",
    "          \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    " \n",
    "        x=self.embedding(x)\n",
    "        out, _ = self.lstm(x, (h0, c0)) \n",
    "        out=self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_1.068%:  Training average Loss: 1.359605\n",
      "Epoch 0_2.136%:  Training average Loss: 1.322123\n",
      "Epoch 0_3.204%:  Training average Loss: 1.290417\n",
      "Epoch 0_4.272%:  Training average Loss: 1.267840\n",
      "Epoch 0_5.340%:  Training average Loss: 1.261175\n",
      "Epoch 0_6.408%:  Training average Loss: 1.249108\n",
      "Epoch 0_7.476%:  Training average Loss: 1.244924\n",
      "Epoch 0_8.544%:  Training average Loss: 1.238481\n",
      "Epoch 0_9.612%:  Training average Loss: 1.237244\n",
      "Epoch 0_10.680%:  Training average Loss: 1.235349\n",
      "Epoch 0_11.748%:  Training average Loss: 1.233563\n",
      "Epoch 0_12.816%:  Training average Loss: 1.232846\n",
      "Epoch 0_13.884%:  Training average Loss: 1.231464\n",
      "Epoch 0_14.952%:  Training average Loss: 1.230923\n",
      "Epoch 0_16.019%:  Training average Loss: 1.228025\n",
      "Epoch 0_17.087%:  Training average Loss: 1.225674\n",
      "Epoch 0_18.155%:  Training average Loss: 1.221756\n",
      "Epoch 0_19.223%:  Training average Loss: 1.217847\n",
      "Epoch 0_20.291%:  Training average Loss: 1.216206\n",
      "Epoch 0_21.359%:  Training average Loss: 1.215820\n",
      "Epoch 0_22.427%:  Training average Loss: 1.213487\n",
      "Epoch 0_23.495%:  Training average Loss: 1.211474\n",
      "Epoch 0_24.563%:  Training average Loss: 1.210957\n",
      "Epoch 0_25.631%:  Training average Loss: 1.211454\n",
      "Epoch 0_26.699%:  Training average Loss: 1.210816\n",
      "Epoch 0_27.767%:  Training average Loss: 1.208213\n",
      "Epoch 0_28.835%:  Training average Loss: 1.207387\n",
      "Epoch 0_29.903%:  Training average Loss: 1.207046\n",
      "Epoch 0_30.971%:  Training average Loss: 1.206547\n",
      "Epoch 0_32.039%:  Training average Loss: 1.204678\n",
      "Epoch 0_33.107%:  Training average Loss: 1.201715\n",
      "Epoch 0_34.175%:  Training average Loss: 1.202257\n",
      "Epoch 0_35.243%:  Training average Loss: 1.200443\n",
      "Epoch 0_36.311%:  Training average Loss: 1.199049\n",
      "Epoch 0_37.379%:  Training average Loss: 1.198023\n",
      "Epoch 0_38.447%:  Training average Loss: 1.195794\n",
      "Epoch 0_39.515%:  Training average Loss: 1.195012\n",
      "Epoch 0_40.583%:  Training average Loss: 1.194548\n",
      "Epoch 0_41.651%:  Training average Loss: 1.191882\n",
      "Epoch 0_42.719%:  Training average Loss: 1.189998\n",
      "Epoch 0_43.787%:  Training average Loss: 1.189647\n",
      "Epoch 0_44.855%:  Training average Loss: 1.188345\n",
      "Epoch 0_45.923%:  Training average Loss: 1.186996\n",
      "Epoch 0_46.990%:  Training average Loss: 1.185665\n",
      "Epoch 0_48.058%:  Training average Loss: 1.183963\n",
      "Epoch 0_49.126%:  Training average Loss: 1.182256\n",
      "Epoch 0_50.194%:  Training average Loss: 1.180369\n",
      "Epoch 0_51.262%:  Training average Loss: 1.179926\n",
      "Epoch 0_52.330%:  Training average Loss: 1.177762\n",
      "Epoch 0_53.398%:  Training average Loss: 1.175905\n",
      "Epoch 0_54.466%:  Training average Loss: 1.173483\n",
      "Epoch 0_55.534%:  Training average Loss: 1.171750\n",
      "Epoch 0_56.602%:  Training average Loss: 1.169812\n",
      "Epoch 0_57.670%:  Training average Loss: 1.168276\n",
      "Epoch 0_58.738%:  Training average Loss: 1.166475\n",
      "Epoch 0_59.806%:  Training average Loss: 1.164482\n",
      "Epoch 0_60.874%:  Training average Loss: 1.162078\n",
      "Epoch 0_61.942%:  Training average Loss: 1.160359\n",
      "Epoch 0_63.010%:  Training average Loss: 1.158874\n",
      "Epoch 0_64.078%:  Training average Loss: 1.157909\n",
      "Epoch 0_65.146%:  Training average Loss: 1.156137\n",
      "Epoch 0_66.214%:  Training average Loss: 1.153502\n",
      "Epoch 0_67.282%:  Training average Loss: 1.151773\n",
      "Epoch 0_68.350%:  Training average Loss: 1.150482\n",
      "Epoch 0_69.418%:  Training average Loss: 1.148638\n",
      "Epoch 0_70.486%:  Training average Loss: 1.147008\n",
      "Epoch 0_71.554%:  Training average Loss: 1.145392\n",
      "Epoch 0_72.622%:  Training average Loss: 1.144733\n",
      "Epoch 0_73.690%:  Training average Loss: 1.143706\n",
      "Epoch 0_74.758%:  Training average Loss: 1.142845\n",
      "Epoch 0_75.826%:  Training average Loss: 1.141798\n",
      "Epoch 0_76.894%:  Training average Loss: 1.139872\n",
      "Epoch 0_77.961%:  Training average Loss: 1.138290\n",
      "Epoch 0_79.029%:  Training average Loss: 1.136744\n",
      "Epoch 0_80.097%:  Training average Loss: 1.135371\n",
      "Epoch 0_81.165%:  Training average Loss: 1.134117\n",
      "Epoch 0_82.233%:  Training average Loss: 1.132624\n",
      "Epoch 0_83.301%:  Training average Loss: 1.131961\n",
      "Epoch 0_84.369%:  Training average Loss: 1.130516\n",
      "Epoch 0_85.437%:  Training average Loss: 1.129282\n",
      "Epoch 0_86.505%:  Training average Loss: 1.127913\n",
      "Epoch 0_87.573%:  Training average Loss: 1.126635\n",
      "Epoch 0_88.641%:  Training average Loss: 1.125907\n",
      "Epoch 0_89.709%:  Training average Loss: 1.123936\n",
      "Epoch 0_90.777%:  Training average Loss: 1.122768\n",
      "Epoch 0_91.845%:  Training average Loss: 1.121959\n",
      "Epoch 0_92.913%:  Training average Loss: 1.120715\n",
      "Epoch 0_93.981%:  Training average Loss: 1.119123\n",
      "Epoch 0_95.049%:  Training average Loss: 1.117982\n",
      "Epoch 0_96.117%:  Training average Loss: 1.116978\n",
      "Epoch 0_97.185%:  Training average Loss: 1.115601\n",
      "Epoch 0_98.253%:  Training average Loss: 1.114672\n",
      "Epoch 0_99.321%:  Training average Loss: 1.113815\n",
      "Epoch 0 :  Verification average Loss: 1.007660, Verification accuracy: 58.176342%,Total Time:246.045965\n",
      "Model is saved in model_dict/model_glove/epoch_0_accuracy_0.581763\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epoch = 1\n",
    "best_accuracy = 0.0\n",
    "start_time = time.time()\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        steps+=1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_text = batch.Phrase\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch_text)\n",
    "        loss = criterion(out,batch_label)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct = (torch.max(out,dim=1)[1].view(batch_label.size())==batch_label).sum()\n",
    "        total_correct+=correct.item()\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                      %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps))\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(dev_iterator.dataset)\n",
    "    steps = 0.0    \n",
    "    for batch in dev_iterator:\n",
    "        steps+=1\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "        \n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "          %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  \n",
    "        \n",
    "        if best_accuracy < total_correct/total_data_num :\n",
    "            best_accuracy =total_correct/total_data_num \n",
    "            torch.save(model,'model_dict/model_lstm/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            print('Model is saved in model_dict/model_lstm/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average Loss: 1.009824, Test accuracy: 0.194188，Total time: 23.237828\n"
     ]
    }
   ],
   "source": [
    "PATH='model_dict/model_lstm/epoch_0_accuracy_0.581763'\n",
    "model = torch.load(PATH)\n",
    "total_loss=0.0\n",
    "accuracy=0.0\n",
    "total_correct=0.0\n",
    "total_data_num = len(train_iterator.dataset)\n",
    "steps = 0.0    \n",
    "start_time=time.time()\n",
    "for batch in test_iterator:\n",
    "    steps+=1\n",
    "    batch_text=batch.Phrase\n",
    "    batch_label=batch.Sentiment\n",
    "    out=model(batch_text)\n",
    "    loss = criterion(out, batch_label)\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "    total_correct = total_correct + correct.item()\n",
    "    #break   \n",
    "\n",
    "print(\"Test average Loss: %f, Test accuracy: %f，Total time: %f\"\n",
    "  %(total_loss/steps, total_correct/total_data_num,time.time()-start_time) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='model_dict/model_lstm/epoch_0_accuracy_0.581763'\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicts=[]\n",
    "    for batch in pred_iterator:\n",
    "        batch_text=batch.Phrase\n",
    "        out=model(batch_text)\n",
    "        predicts.extend(out.argmax(1).cpu().numpy())\n",
    "    \n",
    "    test_data=pd.read_csv(\"test.tsv\",sep='\\t')\n",
    "    test_data[\"Sentiment\"]=predicts\n",
    "    test_data[['PhraseId','Sentiment']].set_index('PhraseId').to_csv('rnn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
